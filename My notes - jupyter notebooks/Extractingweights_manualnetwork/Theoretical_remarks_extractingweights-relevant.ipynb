{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3.5: Extracting Weights and Manual Network Calculation\n",
    "\n",
    "### Weight Initialization\n",
    "\n",
    "The weights of a neural network determine the output for the neural network.  The process of training can adjust these weights so the neural network produces useful output.  Most neural network training algorithms begin by initializing the weights to a random state.  Training then progresses through a series of iterations that continuously improve the weights to produce better output.\n",
    "\n",
    "The random weights of a neural network impact how well that neural network can be trained.  If a neural network fails to train, you can remedy the problem by simply restarting with a new set of random weights. However, this solution can be frustrating when you are experimenting with the architecture of a neural network and trying different combinations of hidden layers and neurons.  If you add a new layer, and the networkâ€™s performance improves, you must ask yourself if this improvement resulted from the new layer or from a new set of weights.  Because of this uncertainty, we look for two key attributes in a weight initialization algorithm:\n",
    "\n",
    "* How consistently does this algorithm provide good weights?\n",
    "* How much of an advantage do the weights of the algorithm provide?\n",
    "\n",
    "One of the most common, yet least effective, approaches to weight initialization is to set the weights to random values within a specific range.  Numbers between -1 and +1 or -5 and +5 are often the choice.  If you want to ensure that you get the same set of random weights each time, you should use a seed.  The seed specifies a set of predefined random weights to use.  For example, a seed of 1000 might produce random weights of 0.5, 0.75, and 0.2. These values are still random; you cannot predict them, yet you will always get these values when you choose a seed of 1000. \n",
    "Not all seeds are created equal.  One problem with random weight initialization is that the random weights created by some seeds are much more difficult to train than others.  In fact, the weights can be so bad that training is impossible.  If you find that you cannot train a neural network with a particular weight set, you should generate a new set of weights using a different seed.\n",
    "\n",
    "Because weight initialization is a problem, there has been considerable research around it.  In this course we use the Xavier weight initialization algorithm, introduced in 2006 by Glorot & Bengio[[Cite:glorot2010understanding]](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), produces good weights with reasonable consistency.  This relatively simple algorithm uses normally distributed random numbers.  \n",
    "\n",
    "To use the Xavier weight initialization, it is necessary to understand that normally distributed random numbers are not the typical random numbers between 0 and 1 that most programming languages generate.  In fact, normally distributed random numbers are centered on a mean ($\\mu$, mu) that is typically 0.  If 0 is the center (mean), then you will get an equal number of random numbers above and below 0.  The next question is how far these random numbers will venture from 0.  In theory, you could end up with both positive and negative numbers close to the maximum positive and negative ranges supported by your computer.  However, the reality is that you will more likely see random numbers that are between 0 and three standard deviations from the center.\n",
    "\n",
    "The standard deviation ($\\sigma$, sigma) parameter specifies the size of this standard deviation.  For example, if you specified a standard deviation of 10, then you would mainly see random numbers between -30 and +30, and the numbers nearer to 0 have a much higher probability of being selected.  \n",
    "\n",
    "The above figure illustrates that the center, which in this case is 0, will be generated with a 0.4 (40%) probability.  Additionally, the probability decreases very quickly beyond -2 or +2 standard deviations. By defining the center and how large the standard deviations are, you are able to control the range of random numbers that you will receive.\n",
    "\n",
    "The Xavier weight initialization sets all of the weights to normally distributed random numbers.  These weights are always centered at 0; however, their standard deviation varies depending on how many connections are present for the current layer of weights.  Specifically, Equation 4.2 can determine the standard deviation:\n",
    "\n",
    "$ Var(W) = \\frac{2}{n_{in}+n_{out}} $\n",
    "\n",
    "The above equation shows how to obtain the variance for all of the weights.  The square root of the variance is the standard deviation.  Most random number generators accept a standard deviation rather than a variance.  As a result, you usually need to take the square root of the above equation.  Figure 3.XAVIER shows how one layer might be initialized. \n",
    "\n",
    "**Figure 3.XAVIER: Xavier Weight Initialization**\n",
    "<img src=\"xavier_weight.png\"  style=\"width: 150px;\"/>\n",
    "\n",
    "This process is completed for each layer in the neural network.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The weights determine the result of the neural network, what we really train are the weights, here 'train' means \n",
    "adjusting the weights.\n",
    "\n",
    "To initialize the algorithm we use random weights. Some weights can be better than others \n",
    "in order to initialize the algorithm. \n",
    "\n",
    "The initial weights conditionate the result of the neural network. If a neural network fails\n",
    "to train, we can restart the neural network with different starting weights.\n",
    "\n",
    "Sometimes you are improving your neural network, for instance adding hidden layers or neurons, and you cannot\n",
    "differenciate if the improvement is coming from the neural network architecture or from the initial weight.\n",
    "\n",
    "This is pointing us that we need a initialization algorithm. We look for two key attributes in a weight initialization algorithm:\n",
    "\n",
    "* How consistently does this algorithm provide good weights?\n",
    "* How much of an advantage do the weights of the algorithm provide?\n",
    "\n",
    "One of the most common, yet least effective, approaches to weight initialization is to set the weights to random values within a specific range.  Numbers between -1 and +1 or -5 and +5 are often the choice.  If you want to ensure that you get the same set of random weights each time, you should use a seed. \n",
    "\n",
    "Not all seeds are created equal.  One problem with random weight initialization is that the random weights created by some seeds are much more difficult to train than others.  In fact, the weights can be so bad that training is impossible.  If you find that you cannot train a neural network with a particular weight set, you should generate a new set of weights using a different seed.\n",
    "\n",
    "Because weight initialization is a problem, there has been considerable research around it.  In this course we use the Xavier weight initialization algorithm, introduced in 2006 by Glorot & Bengio[[Cite:glorot2010understanding]](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), produces good weights with reasonable consistency.  This relatively simple algorithm uses normally distributed random numbers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Neural Network Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple neural network that learn the XOR, exclusive or inclusive. For simplicity we use Keras to train the network for us. The neural network is small.  Two inputs, two hidden neurons, and a single output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycle #1\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff970670820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[0.49999997]\n",
      " [0.49999997]\n",
      " [0.49999997]\n",
      " [0.49999997]]\n",
      "Cycle #2\n",
      "WARNING:tensorflow:8 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff9009a60d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[0.49999997]\n",
      " [0.49999997]\n",
      " [0.49999997]\n",
      " [0.49999997]]\n",
      "Cycle #3\n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff943830820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[4.9999988e-01]\n",
      " [1.0000000e+00]\n",
      " [4.9999988e-01]\n",
      " [1.1920929e-07]]\n",
      "Cycle #4\n",
      "WARNING:tensorflow:10 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff9008f9ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[0.3333333 ]\n",
      " [0.99999994]\n",
      " [0.3333333 ]\n",
      " [0.3333333 ]]\n",
      "Cycle #5\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff910857790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[4.9999985e-01]\n",
      " [1.0000000e+00]\n",
      " [4.9999985e-01]\n",
      " [1.1920929e-07]]\n",
      "Cycle #6\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff9603e4b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[9.4441020e-08]\n",
      " [9.9999988e-01]\n",
      " [9.9999988e-01]\n",
      " [1.6084614e-07]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "\n",
    "# Create a dataset for the XOR function\n",
    "x = np.array([\n",
    "    [0,0],\n",
    "    [1,0],\n",
    "    [0,1],\n",
    "    [1,1]\n",
    "]) # predictor\n",
    "\n",
    "y = np.array([\n",
    "    0,\n",
    "    1,\n",
    "    1,\n",
    "    0\n",
    "]) # dependent variable\n",
    "\n",
    "# Build the network\n",
    "# sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "done = False\n",
    "cycle = 1\n",
    "\n",
    "while not done:\n",
    "    print(\"Cycle #{}\".format(cycle))\n",
    "    cycle+=1\n",
    "    model = Sequential()\n",
    "    model.add(Dense(2, input_dim=2, activation='relu')) \n",
    "    model.add(Dense(1)) \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(x,y,verbose=0,epochs=10000)\n",
    "\n",
    "    # Predict\n",
    "    pred = model.predict(x)\n",
    "    \n",
    "    # Check if successful.  It takes several runs with this \n",
    "    # small of a network\n",
    "    done = pred[0]<0.01 and pred[3]<0.01 and pred[1] > 0.9 \\\n",
    "        and pred[2] > 0.9 \n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0B -> L1N0: -1.2041040658950806\n",
      "0B -> L1N1: 7.557218140163968e-08\n",
      "L0N0                   -> L1N0 = 1.2041043043136597\n",
      "L0N0                   -> L1N1 = 1.4311785697937012\n",
      "L0N1                   -> L1N0 = 1.2041040658950806\n",
      "L0N1                   -> L1N1 = 1.4311779737472534\n",
      "1B -> L2N0: 4.163685574098963e-08\n",
      "L1N0                   -> L2N0 = -1.6609855890274048\n",
      "L1N1                   -> L2N0 = 0.6987249255180359\n"
     ]
    }
   ],
   "source": [
    "# Dump weights\n",
    "for layerNum, layer in enumerate(model.layers):\n",
    "    weights = layer.get_weights()[0]\n",
    "    biases = layer.get_weights()[1]\n",
    "    \n",
    "    for toNeuronNum, bias in enumerate(biases):\n",
    "        print(f'{layerNum}B -> L{layerNum+1}N{toNeuronNum}: {bias}')\n",
    "    \n",
    "    for fromNeuronNum, wgt in enumerate(weights):\n",
    "        for toNeuronNum, wgt2 in enumerate(wgt):\n",
    "            print(f'L{layerNum}N{fromNeuronNum} \\\n",
    "                  -> L{layerNum+1}N{toNeuronNum} = {wgt2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.2\n",
      "0\n",
      "1.2\n",
      "0.96\n",
      "0.96\n"
     ]
    }
   ],
   "source": [
    "input0 = 0\n",
    "input1 = 1\n",
    "\n",
    "hidden0Sum = (input0*1.3)+(input1*1.3)+(-1.3)\n",
    "hidden1Sum = (input0*1.2)+(input1*1.2)+(0)\n",
    "\n",
    "print(hidden0Sum) # 0\n",
    "print(hidden1Sum) # 1.2\n",
    "\n",
    "hidden0 = max(0,hidden0Sum)\n",
    "hidden1 = max(0,hidden1Sum)\n",
    "\n",
    "print(hidden0) # 0\n",
    "print(hidden1) # 1.2\n",
    "\n",
    "outputSum = (hidden0*-1.6)+(hidden1*0.8)+(0)\n",
    "print(outputSum) # 0.96\n",
    "\n",
    "output = max(0,outputSum)\n",
    "\n",
    "print(output) # 0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
