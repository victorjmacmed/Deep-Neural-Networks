{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Linear Algebra Examples\n",
    "\n",
    "These are some notes about TensorFlow. These notes are a modified version from the note of a course in Deep Neural Networks at Washington University in St. Louis.\n",
    "\n",
    "I added clarifications and some extra examples to this code. I also added explanations and used different concepts to test and exemplify the use of TensorFlow and Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[10. 10.]\n",
      "  [10. 10.]]\n",
      "\n",
      " [[10. 10.]\n",
      "  [10. 10.]]\n",
      "\n",
      " [[10. 10.]\n",
      "  [10. 10.]]], shape=(3, 2, 2), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([10., 10.], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "matrix1 = tf.constant([[3., 3.]]) \n",
    "matrix2 = tf.constant([[2.],[2.]])\n",
    "\n",
    "# To clarify this. In matrix1 we are creating a tensor with constant entries.\n",
    "# In this particular case, we create a shape = (1,2) tensor, that is, a \n",
    "# 1x2 matrix or a vector of dimension 2.\n",
    "#\n",
    "# For matrix2 we create a shape = (2,1) tensor, that is, a 2x1 matrix which is a \n",
    "# column vector.\n",
    "#\n",
    "# Just to exemplify a shape = (3,2,2) tensor is like a cube with 3, \n",
    "# 2x2 matrices inside.\n",
    "# \n",
    "\n",
    "A = tf.constant(10.0, shape = (3,2,2))\n",
    "\n",
    "print(A) # three layers with 2x2 constant matrices at each one.\n",
    "\n",
    "# The first coordinate of shape give us the number of layers.\n",
    "\n",
    "A[1][0,:] # we access matrix 1 and then we access row 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[12.]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiplication of two vectors using TensorFlow\n",
    "\n",
    "product = tf.matmul(matrix1, matrix2) # Here we multiply two vectors - classic multiplication\n",
    "product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 2), dtype=float32, numpy=\n",
       "array([[[60., 60.]],\n",
       "\n",
       "       [[60., 60.]],\n",
       "\n",
       "       [[60., 60.]]], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(matrix1,A) # Here we multiply a vector by a tensor\n",
    "                     # We get the multiplication of this vector\n",
    "                     # with each matrix inside the tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Substract a constant from a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-2. -2.], shape=(2,), dtype=float32)\n",
      "[-2. -2.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4., 6.], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Important information about TensorFlow variables and constant variables.\n",
    "\n",
    "x = tf.Variable([1.0, 1.0]) # creating tensors in this case with shape (0,2,1)\n",
    "                            # This is a vector.\n",
    "    \n",
    "a = tf.constant([3.0, 3.0])\n",
    "\n",
    "sub = tf.subtract(x, a) \n",
    "print(sub)\n",
    "print(sub.numpy())\n",
    "\n",
    "# I think that variables can be modified but the other 'Python' var we are \n",
    "# definning here cannot be modified (for instance variable a).\n",
    "#\n",
    "# There is a subtle but important difference between Python and TensorFlow\n",
    "# variables.  TensorFlow variables can be modified as written above.\n",
    "\n",
    "# To modify one of these variables we use the command assign\n",
    "# or any OP command\n",
    "\n",
    "x.assign([4.0,6.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 3.], shape=(2,), dtype=float32)\n",
      "[1. 3.]\n"
     ]
    }
   ],
   "source": [
    "# substraction with this new value\n",
    "\n",
    "sub = tf.subtract(x,a)\n",
    "print(sub)\n",
    "print(sub.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Keras - Neural Networks\n",
    "\n",
    "Keras is part of TensorFlow. It is useful to create neural networks. \n",
    "We will use a particular data set from Heaton's website. This dataset \n",
    "is called MPG.\n",
    "\n",
    "<!-- - Predictors/Inputs\n",
    "    - Fill any missing inputs with the median for that column. Use missing_median.\n",
    "    - Encode textual/categorical values with encode_text_dummy.\n",
    "    - Encode numeric values with encode_numeric_zscore.\n",
    "- Output\n",
    "    - Discard rows with missing outputs.\n",
    "    - Encode textual/categorical values with encode_text_index.\n",
    "    - Do not encode output numeric values.\n",
    "- Produce final feature vectors (x) and expected output (y) with to_xy.-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 - 0s - loss: 39998.6953\n",
      "Epoch 2/100\n",
      "13/13 - 0s - loss: 3936.8127\n",
      "Epoch 3/100\n",
      "13/13 - 0s - loss: 2445.1257\n",
      "Epoch 4/100\n",
      "13/13 - 0s - loss: 1171.2573\n",
      "Epoch 5/100\n",
      "13/13 - 0s - loss: 822.6342\n",
      "Epoch 6/100\n",
      "13/13 - 0s - loss: 765.8757\n",
      "Epoch 7/100\n",
      "13/13 - 0s - loss: 729.7697\n",
      "Epoch 8/100\n",
      "13/13 - 0s - loss: 704.8073\n",
      "Epoch 9/100\n",
      "13/13 - 0s - loss: 681.6252\n",
      "Epoch 10/100\n",
      "13/13 - 0s - loss: 662.3399\n",
      "Epoch 11/100\n",
      "13/13 - 0s - loss: 640.6265\n",
      "Epoch 12/100\n",
      "13/13 - 0s - loss: 622.9199\n",
      "Epoch 13/100\n",
      "13/13 - 0s - loss: 603.0743\n",
      "Epoch 14/100\n",
      "13/13 - 0s - loss: 583.1974\n",
      "Epoch 15/100\n",
      "13/13 - 0s - loss: 559.0380\n",
      "Epoch 16/100\n",
      "13/13 - 0s - loss: 541.9359\n",
      "Epoch 17/100\n",
      "13/13 - 0s - loss: 525.6568\n",
      "Epoch 18/100\n",
      "13/13 - 0s - loss: 499.7672\n",
      "Epoch 19/100\n",
      "13/13 - 0s - loss: 482.8459\n",
      "Epoch 20/100\n",
      "13/13 - 0s - loss: 469.9066\n",
      "Epoch 21/100\n",
      "13/13 - 0s - loss: 442.9878\n",
      "Epoch 22/100\n",
      "13/13 - 0s - loss: 425.1157\n",
      "Epoch 23/100\n",
      "13/13 - 0s - loss: 405.0152\n",
      "Epoch 24/100\n",
      "13/13 - 0s - loss: 388.6633\n",
      "Epoch 25/100\n",
      "13/13 - 0s - loss: 370.7226\n",
      "Epoch 26/100\n",
      "13/13 - 0s - loss: 354.9890\n",
      "Epoch 27/100\n",
      "13/13 - 0s - loss: 339.1622\n",
      "Epoch 28/100\n",
      "13/13 - 0s - loss: 322.5049\n",
      "Epoch 29/100\n",
      "13/13 - 0s - loss: 305.4431\n",
      "Epoch 30/100\n",
      "13/13 - 0s - loss: 295.7998\n",
      "Epoch 31/100\n",
      "13/13 - 0s - loss: 280.9726\n",
      "Epoch 32/100\n",
      "13/13 - 0s - loss: 262.3723\n",
      "Epoch 33/100\n",
      "13/13 - 0s - loss: 254.2515\n",
      "Epoch 34/100\n",
      "13/13 - 0s - loss: 238.5047\n",
      "Epoch 35/100\n",
      "13/13 - 0s - loss: 225.1913\n",
      "Epoch 36/100\n",
      "13/13 - 0s - loss: 212.1149\n",
      "Epoch 37/100\n",
      "13/13 - 0s - loss: 204.2687\n",
      "Epoch 38/100\n",
      "13/13 - 0s - loss: 188.1229\n",
      "Epoch 39/100\n",
      "13/13 - 0s - loss: 179.4707\n",
      "Epoch 40/100\n",
      "13/13 - 0s - loss: 173.3159\n",
      "Epoch 41/100\n",
      "13/13 - 0s - loss: 159.4192\n",
      "Epoch 42/100\n",
      "13/13 - 0s - loss: 149.4134\n",
      "Epoch 43/100\n",
      "13/13 - 0s - loss: 142.7071\n",
      "Epoch 44/100\n",
      "13/13 - 0s - loss: 135.3130\n",
      "Epoch 45/100\n",
      "13/13 - 0s - loss: 129.1866\n",
      "Epoch 46/100\n",
      "13/13 - 0s - loss: 121.2609\n",
      "Epoch 47/100\n",
      "13/13 - 0s - loss: 112.9785\n",
      "Epoch 48/100\n",
      "13/13 - 0s - loss: 107.4336\n",
      "Epoch 49/100\n",
      "13/13 - 0s - loss: 96.9472\n",
      "Epoch 50/100\n",
      "13/13 - 0s - loss: 92.0488\n",
      "Epoch 51/100\n",
      "13/13 - 0s - loss: 85.5888\n",
      "Epoch 52/100\n",
      "13/13 - 0s - loss: 79.8842\n",
      "Epoch 53/100\n",
      "13/13 - 0s - loss: 74.8063\n",
      "Epoch 54/100\n",
      "13/13 - 0s - loss: 71.1676\n",
      "Epoch 55/100\n",
      "13/13 - 0s - loss: 66.6375\n",
      "Epoch 56/100\n",
      "13/13 - 0s - loss: 62.1997\n",
      "Epoch 57/100\n",
      "13/13 - 0s - loss: 58.4264\n",
      "Epoch 58/100\n",
      "13/13 - 0s - loss: 54.5717\n",
      "Epoch 59/100\n",
      "13/13 - 0s - loss: 50.8358\n",
      "Epoch 60/100\n",
      "13/13 - 0s - loss: 47.5859\n",
      "Epoch 61/100\n",
      "13/13 - 0s - loss: 45.4374\n",
      "Epoch 62/100\n",
      "13/13 - 0s - loss: 43.6370\n",
      "Epoch 63/100\n",
      "13/13 - 0s - loss: 41.2131\n",
      "Epoch 64/100\n",
      "13/13 - 0s - loss: 37.7682\n",
      "Epoch 65/100\n",
      "13/13 - 0s - loss: 35.2147\n",
      "Epoch 66/100\n",
      "13/13 - 0s - loss: 32.9514\n",
      "Epoch 67/100\n",
      "13/13 - 0s - loss: 31.0820\n",
      "Epoch 68/100\n",
      "13/13 - 0s - loss: 29.3931\n",
      "Epoch 69/100\n",
      "13/13 - 0s - loss: 28.0737\n",
      "Epoch 70/100\n",
      "13/13 - 0s - loss: 26.5158\n",
      "Epoch 71/100\n",
      "13/13 - 0s - loss: 25.0154\n",
      "Epoch 72/100\n",
      "13/13 - 0s - loss: 23.7621\n",
      "Epoch 73/100\n",
      "13/13 - 0s - loss: 22.5763\n",
      "Epoch 74/100\n",
      "13/13 - 0s - loss: 21.6511\n",
      "Epoch 75/100\n",
      "13/13 - 0s - loss: 21.1141\n",
      "Epoch 76/100\n",
      "13/13 - 0s - loss: 20.6238\n",
      "Epoch 77/100\n",
      "13/13 - 0s - loss: 18.9551\n",
      "Epoch 78/100\n",
      "13/13 - 0s - loss: 18.4969\n",
      "Epoch 79/100\n",
      "13/13 - 0s - loss: 18.0311\n",
      "Epoch 80/100\n",
      "13/13 - 0s - loss: 17.0049\n",
      "Epoch 81/100\n",
      "13/13 - 0s - loss: 16.7528\n",
      "Epoch 82/100\n",
      "13/13 - 0s - loss: 15.9305\n",
      "Epoch 83/100\n",
      "13/13 - 0s - loss: 15.6825\n",
      "Epoch 84/100\n",
      "13/13 - 0s - loss: 15.1671\n",
      "Epoch 85/100\n",
      "13/13 - 0s - loss: 15.0723\n",
      "Epoch 86/100\n",
      "13/13 - 0s - loss: 14.8069\n",
      "Epoch 87/100\n",
      "13/13 - 0s - loss: 14.2256\n",
      "Epoch 88/100\n",
      "13/13 - 0s - loss: 13.9010\n",
      "Epoch 89/100\n",
      "13/13 - 0s - loss: 13.6270\n",
      "Epoch 90/100\n",
      "13/13 - 0s - loss: 13.3766\n",
      "Epoch 91/100\n",
      "13/13 - 0s - loss: 13.3192\n",
      "Epoch 92/100\n",
      "13/13 - 0s - loss: 13.0787\n",
      "Epoch 93/100\n",
      "13/13 - 0s - loss: 12.8681\n",
      "Epoch 94/100\n",
      "13/13 - 0s - loss: 12.7742\n",
      "Epoch 95/100\n",
      "13/13 - 0s - loss: 13.0850\n",
      "Epoch 96/100\n",
      "13/13 - 0s - loss: 13.0094\n",
      "Epoch 97/100\n",
      "13/13 - 0s - loss: 12.8349\n",
      "Epoch 98/100\n",
      "13/13 - 0s - loss: 12.3666\n",
      "Epoch 99/100\n",
      "13/13 - 0s - loss: 12.8315\n",
      "Epoch 100/100\n",
      "13/13 - 0s - loss: 13.3689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc1418b81c0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Simple TensorFlow Regression: MPG dataset.\n",
    "\n",
    "## This data set has some particularities. The input is numerical and \n",
    "## categorial, some of the values are missing values.\n",
    "\n",
    "# Libraries\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# loading the dataset\n",
    "\n",
    "df = pd.read_csv(\"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\", na_values=['NA', '?'])\n",
    "cars = df['name']\n",
    "\n",
    "# filling missing values\n",
    "\n",
    "df['horsepower'] = df['horsepower'].fillna(df.horsepower.median())\n",
    "\n",
    "# Turning this dataframe into a matrix\n",
    "\n",
    "x = df[['cylinders','displacement','horsepower','weight','acceleration','year','origin']].values\n",
    "\n",
    "y = df.mpg.values # we want to predict mpg values from the matrix X. --> this is the important idea.\n",
    "\n",
    "# We want to do regression, x is the predictor variables and y \n",
    "# is the dependent variable.\n",
    "\n",
    "# Building the neural network\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim = x.shape[1], activation = 'relu')) # Hidden 1 - Input, because the input is fixed.\n",
    "model.add(Dense(10, activation='relu')) # Hidden 2\n",
    "model.add(Dense(1)) # Output\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(x,y,verbose=2,epochs=100)\n",
    "\n",
    "# Entry - layer 0\n",
    "# We add 25 neurons hidden 1 - layer 1\n",
    "# We add 10 neurons hidden 2 - layer 2\n",
    "# We add 1 neuron Output - layer 3\n",
    "\n",
    "# These are four layers - entry, hidden 1, hidden 2, output.\n",
    "\n",
    "# epoch takes data for evaluating the trainning/fitting.\n",
    "#\n",
    "# From the documentation\n",
    "#\n",
    "# Epoch: an arbitrary cutoff, generally defined as \"one pass over the entire dataset\", \n",
    "# used to separate training into distinct phases, which is useful for logging and periodic evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Neural Network Hyperparameters.\n",
    "\n",
    "Some ideas about our previous code:\n",
    "\n",
    "1. The network need one input neuron for each column in the dataset (including empty variables etc).\n",
    "2. How do we choose the number of hidden layers and the number of neuros on each layer?\n",
    "   This question has not already a clear answer. These are hyperparameters. This parameters\n",
    "   can affect neural network performance. There are not clearly defined means of setting them.\n",
    "3. More hidden neurons -> more capability to fit complex problems. \n",
    "   Too many neurons --> risk overfitting.\n",
    "   Too few --> risk underfitting\n",
    "4. How many layers you have is algo a hyperparameter. \n",
    "   More neurons -> neural network perform more of its feature engineering and data preprocessing\n",
    "   Risk of this -> expense of tranining times, risk of overfitting.\n",
    "5. In general networks will have a triangle shape, where we have a lot of neurons on the input side \n",
    "   and one, or a small amount of outputs and the output side.\n",
    "   \n",
    "__Controlling the Amount of Output__\n",
    "\n",
    "The program produces one line of output for each training epoch. You can eliminate this output by setting the verbose setting of the fit command:\n",
    "\n",
    "- verbose=0 - No progress output (use with Jupyter if you do not want output)\n",
    "- verbose=1 - Display progress bar, does not work well with Jupyter\n",
    "- verbose=2 - Summary progress output (use with Jupyter if you want to know the loss at each epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Predictions from our previous neural network__\n",
    "\n",
    "Result from a neural network is an array. To see the dimension of this array we can use 'our array name'.shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (398, 1)\n",
      "[[16.94988 ]\n",
      " [15.422278]\n",
      " [16.967306]\n",
      " [16.751364]\n",
      " [16.919622]\n",
      " [11.649421]\n",
      " [11.327064]\n",
      " [11.511054]\n",
      " [10.742103]\n",
      " [14.352759]]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x)\n",
    "print(f\"Shape: {pred.shape}\") # printing the shape\n",
    "print(pred[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to test if the predictions are good or bad. The correct MPG for each car is on the data.\n",
    "First we calculate the Means Square Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4766323548208207\n"
     ]
    }
   ],
   "source": [
    "# Mean square error\n",
    "\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Printing data against the prediction for the 10 first cars__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPG: 18.0 Predicted MPG: [16.94988]\n",
      "MPG: 15.0 Predicted MPG: [15.422278]\n",
      "MPG: 18.0 Predicted MPG: [16.967306]\n",
      "MPG: 16.0 Predicted MPG: [16.751364]\n",
      "MPG: 17.0 Predicted MPG: [16.919622]\n",
      "MPG: 15.0 Predicted MPG: [11.649421]\n",
      "MPG: 14.0 Predicted MPG: [11.327064]\n",
      "MPG: 14.0 Predicted MPG: [11.511054]\n",
      "MPG: 14.0 Predicted MPG: [10.742103]\n",
      "MPG: 15.0 Predicted MPG: [14.352759]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"MPG: {y[i]}\" + f\" Predicted MPG: {pred[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple classification of the Iris data set.\n",
    "\n",
    "Classification: a neural network tries to classify into one or more classes a data set. \n",
    "To evaluate classification networks we study the percentage of items that where classified\n",
    "incorrectly.\n",
    "\n",
    "This is like gradding a multiple answer exam. In this case the output is a classification with a \n",
    "percent giving us a measure of the quality of this classification.\n",
    "\n",
    "\n",
    "Example: exam classification neural network\n",
    "\n",
    "We have a multianswer question with possible answer a,b,c,d. The trained neural network tell us that\n",
    "we have A: 0.1, B: 0.8, C: 0.05, D: 0.05.\n",
    "\n",
    "\n",
    "__Example: data set iris__\n",
    "\n",
    "We predict the species from some data about the flower as sepal longitud, sepal width, petal longitud, petal width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 - 0s - loss: 1.1375\n",
      "Epoch 2/100\n",
      "5/5 - 0s - loss: 0.9281\n",
      "Epoch 3/100\n",
      "5/5 - 0s - loss: 0.8642\n",
      "Epoch 4/100\n",
      "5/5 - 0s - loss: 0.7847\n",
      "Epoch 5/100\n",
      "5/5 - 0s - loss: 0.7098\n",
      "Epoch 6/100\n",
      "5/5 - 0s - loss: 0.6566\n",
      "Epoch 7/100\n",
      "5/5 - 0s - loss: 0.6156\n",
      "Epoch 8/100\n",
      "5/5 - 0s - loss: 0.5732\n",
      "Epoch 9/100\n",
      "5/5 - 0s - loss: 0.5376\n",
      "Epoch 10/100\n",
      "5/5 - 0s - loss: 0.5085\n",
      "Epoch 11/100\n",
      "5/5 - 0s - loss: 0.4794\n",
      "Epoch 12/100\n",
      "5/5 - 0s - loss: 0.4562\n",
      "Epoch 13/100\n",
      "5/5 - 0s - loss: 0.4395\n",
      "Epoch 14/100\n",
      "5/5 - 0s - loss: 0.4210\n",
      "Epoch 15/100\n",
      "5/5 - 0s - loss: 0.4015\n",
      "Epoch 16/100\n",
      "5/5 - 0s - loss: 0.3860\n",
      "Epoch 17/100\n",
      "5/5 - 0s - loss: 0.3726\n",
      "Epoch 18/100\n",
      "5/5 - 0s - loss: 0.3628\n",
      "Epoch 19/100\n",
      "5/5 - 0s - loss: 0.3499\n",
      "Epoch 20/100\n",
      "5/5 - 0s - loss: 0.3389\n",
      "Epoch 21/100\n",
      "5/5 - 0s - loss: 0.3280\n",
      "Epoch 22/100\n",
      "5/5 - 0s - loss: 0.3186\n",
      "Epoch 23/100\n",
      "5/5 - 0s - loss: 0.3096\n",
      "Epoch 24/100\n",
      "5/5 - 0s - loss: 0.3003\n",
      "Epoch 25/100\n",
      "5/5 - 0s - loss: 0.2971\n",
      "Epoch 26/100\n",
      "5/5 - 0s - loss: 0.2859\n",
      "Epoch 27/100\n",
      "5/5 - 0s - loss: 0.2761\n",
      "Epoch 28/100\n",
      "5/5 - 0s - loss: 0.2674\n",
      "Epoch 29/100\n",
      "5/5 - 0s - loss: 0.2621\n",
      "Epoch 30/100\n",
      "5/5 - 0s - loss: 0.2533\n",
      "Epoch 31/100\n",
      "5/5 - 0s - loss: 0.2475\n",
      "Epoch 32/100\n",
      "5/5 - 0s - loss: 0.2405\n",
      "Epoch 33/100\n",
      "5/5 - 0s - loss: 0.2354\n",
      "Epoch 34/100\n",
      "5/5 - 0s - loss: 0.2300\n",
      "Epoch 35/100\n",
      "5/5 - 0s - loss: 0.2235\n",
      "Epoch 36/100\n",
      "5/5 - 0s - loss: 0.2195\n",
      "Epoch 37/100\n",
      "5/5 - 0s - loss: 0.2138\n",
      "Epoch 38/100\n",
      "5/5 - 0s - loss: 0.2071\n",
      "Epoch 39/100\n",
      "5/5 - 0s - loss: 0.2021\n",
      "Epoch 40/100\n",
      "5/5 - 0s - loss: 0.1970\n",
      "Epoch 41/100\n",
      "5/5 - 0s - loss: 0.1936\n",
      "Epoch 42/100\n",
      "5/5 - 0s - loss: 0.1902\n",
      "Epoch 43/100\n",
      "5/5 - 0s - loss: 0.1853\n",
      "Epoch 44/100\n",
      "5/5 - 0s - loss: 0.1809\n",
      "Epoch 45/100\n",
      "5/5 - 0s - loss: 0.1763\n",
      "Epoch 46/100\n",
      "5/5 - 0s - loss: 0.1720\n",
      "Epoch 47/100\n",
      "5/5 - 0s - loss: 0.1708\n",
      "Epoch 48/100\n",
      "5/5 - 0s - loss: 0.1641\n",
      "Epoch 49/100\n",
      "5/5 - 0s - loss: 0.1633\n",
      "Epoch 50/100\n",
      "5/5 - 0s - loss: 0.1592\n",
      "Epoch 51/100\n",
      "5/5 - 0s - loss: 0.1586\n",
      "Epoch 52/100\n",
      "5/5 - 0s - loss: 0.1557\n",
      "Epoch 53/100\n",
      "5/5 - 0s - loss: 0.1539\n",
      "Epoch 54/100\n",
      "5/5 - 0s - loss: 0.1487\n",
      "Epoch 55/100\n",
      "5/5 - 0s - loss: 0.1485\n",
      "Epoch 56/100\n",
      "5/5 - 0s - loss: 0.1405\n",
      "Epoch 57/100\n",
      "5/5 - 0s - loss: 0.1405\n",
      "Epoch 58/100\n",
      "5/5 - 0s - loss: 0.1365\n",
      "Epoch 59/100\n",
      "5/5 - 0s - loss: 0.1359\n",
      "Epoch 60/100\n",
      "5/5 - 0s - loss: 0.1309\n",
      "Epoch 61/100\n",
      "5/5 - 0s - loss: 0.1283\n",
      "Epoch 62/100\n",
      "5/5 - 0s - loss: 0.1276\n",
      "Epoch 63/100\n",
      "5/5 - 0s - loss: 0.1279\n",
      "Epoch 64/100\n",
      "5/5 - 0s - loss: 0.1213\n",
      "Epoch 65/100\n",
      "5/5 - 0s - loss: 0.1250\n",
      "Epoch 66/100\n",
      "5/5 - 0s - loss: 0.1176\n",
      "Epoch 67/100\n",
      "5/5 - 0s - loss: 0.1215\n",
      "Epoch 68/100\n",
      "5/5 - 0s - loss: 0.1158\n",
      "Epoch 69/100\n",
      "5/5 - 0s - loss: 0.1148\n",
      "Epoch 70/100\n",
      "5/5 - 0s - loss: 0.1121\n",
      "Epoch 71/100\n",
      "5/5 - 0s - loss: 0.1117\n",
      "Epoch 72/100\n",
      "5/5 - 0s - loss: 0.1133\n",
      "Epoch 73/100\n",
      "5/5 - 0s - loss: 0.1110\n",
      "Epoch 74/100\n",
      "5/5 - 0s - loss: 0.1069\n",
      "Epoch 75/100\n",
      "5/5 - 0s - loss: 0.1067\n",
      "Epoch 76/100\n",
      "5/5 - 0s - loss: 0.1077\n",
      "Epoch 77/100\n",
      "5/5 - 0s - loss: 0.1055\n",
      "Epoch 78/100\n",
      "5/5 - 0s - loss: 0.1047\n",
      "Epoch 79/100\n",
      "5/5 - 0s - loss: 0.1054\n",
      "Epoch 80/100\n",
      "5/5 - 0s - loss: 0.0998\n",
      "Epoch 81/100\n",
      "5/5 - 0s - loss: 0.1013\n",
      "Epoch 82/100\n",
      "5/5 - 0s - loss: 0.0985\n",
      "Epoch 83/100\n",
      "5/5 - 0s - loss: 0.0976\n",
      "Epoch 84/100\n",
      "5/5 - 0s - loss: 0.0967\n",
      "Epoch 85/100\n",
      "5/5 - 0s - loss: 0.0958\n",
      "Epoch 86/100\n",
      "5/5 - 0s - loss: 0.0930\n",
      "Epoch 87/100\n",
      "5/5 - 0s - loss: 0.0966\n",
      "Epoch 88/100\n",
      "5/5 - 0s - loss: 0.0920\n",
      "Epoch 89/100\n",
      "5/5 - 0s - loss: 0.0951\n",
      "Epoch 90/100\n",
      "5/5 - 0s - loss: 0.0915\n",
      "Epoch 91/100\n",
      "5/5 - 0s - loss: 0.0907\n",
      "Epoch 92/100\n",
      "5/5 - 0s - loss: 0.0909\n",
      "Epoch 93/100\n",
      "5/5 - 0s - loss: 0.0905\n",
      "Epoch 94/100\n",
      "5/5 - 0s - loss: 0.0878\n",
      "Epoch 95/100\n",
      "5/5 - 0s - loss: 0.0880\n",
      "Epoch 96/100\n",
      "5/5 - 0s - loss: 0.0884\n",
      "Epoch 97/100\n",
      "5/5 - 0s - loss: 0.0860\n",
      "Epoch 98/100\n",
      "5/5 - 0s - loss: 0.0852\n",
      "Epoch 99/100\n",
      "5/5 - 0s - loss: 0.0863\n",
      "Epoch 100/100\n",
      "5/5 - 0s - loss: 0.0837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc111ab3430>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# loading the data from Heaton's website\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/iris.csv\", \n",
    "    na_values=['NA', '?'])\n",
    "\n",
    "# Getting a matrix from a data frame\n",
    "\n",
    "y = df[['sepal_l','sepal_w', 'petal_l', 'petal_w']].values\n",
    "dummies = pd.get_dummies(df['species']) # Convert categorical variable into dummy/indicator variables.\n",
    "                                        # Classification\n",
    "species = dummies.columns # this creates a variable which gives you back the species from the dummy label\n",
    "y = dummies.values # this creates an array with 0,1,2 indicating the species\n",
    "\n",
    "# Build neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=x.shape[1], activation='relu')) # Hidden 1, 50 neurons - layer 2\n",
    "model.add(Dense(25, activation='relu')) # Hidden 2, 25 neurons, layer 3\n",
    "model.add(Dense(y.shape[1],activation='softmax')) # Output, 1 output neuron, layer 4\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(x,y,verbose=2,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (150, 3)\n",
      "[[9.9791592e-01 2.0840375e-03 5.5303282e-08]\n",
      " [9.9431568e-01 5.6840456e-03 2.5744444e-07]\n",
      " [9.9660110e-01 3.3986005e-03 1.9268818e-07]\n",
      " [9.9270284e-01 7.2965850e-03 5.4237574e-07]\n",
      " [9.9813288e-01 1.8671430e-03 5.6669318e-08]\n",
      " [9.9828738e-01 1.7125459e-03 3.4645893e-08]\n",
      " [9.9696296e-01 3.0367514e-03 2.2351551e-07]\n",
      " [9.9676466e-01 3.2352540e-03 1.0839280e-07]\n",
      " [9.9114662e-01 8.8524753e-03 9.6549502e-07]\n",
      " [9.9377370e-01 6.2260390e-03 2.4102863e-07]]\n"
     ]
    }
   ],
   "source": [
    "# Predictions using our neural network (first 10 values)\n",
    "\n",
    "pred = model.predict(x)\n",
    "print(f\"Shape: {pred.shape}\")\n",
    "print(pred[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9979159  0.00208404 0.00000006]\n",
      " [0.9943157  0.00568405 0.00000026]\n",
      " [0.9966011  0.0033986  0.00000019]\n",
      " [0.99270284 0.00729659 0.00000054]\n",
      " [0.9981329  0.00186714 0.00000006]\n",
      " [0.9982874  0.00171255 0.00000003]\n",
      " [0.99696296 0.00303675 0.00000022]\n",
      " [0.99676466 0.00323525 0.00000011]\n",
      " [0.9911466  0.00885248 0.00000097]\n",
      " [0.9937737  0.00622604 0.00000024]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True) # deactivating scientific notation\n",
    "print(pred[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Real values\n",
    "\n",
    "print(y[0:10])\n",
    "\n",
    "np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the program considers the column with highest prediction to be the prediction of the neural network.\n",
    "The argmax function finds the index of the maximum prediction for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "Expected: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "predict_classes = np.argmax(pred,axis=1) # np.argmax returns the indices of the maximum values along an axis.\n",
    "expected_classes = np.argmax(y,axis=1)\n",
    "print(f\"Predictions: {predict_classes}\")\n",
    "print(f\"Expected: {expected_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
      "       'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
      "       'Iris-setosa'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Turning this back into species\n",
    "\n",
    "print(species[predict_classes[1:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9866666666666667\n"
     ]
    }
   ],
   "source": [
    "# Testing the result. For all of the iris predictions, what percent were correct?\n",
    "#\n",
    "# We want to study accuracy. -> accuracy does not consider how confident the neural network was in each prediction.\n",
    "# Accuracy is a global measure\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "correct = accuracy_score(expected_classes,predict_classes)\n",
    "print(f\"Accuracy: {correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_l</th>\n",
       "      <th>sepal_w</th>\n",
       "      <th>petal_l</th>\n",
       "      <th>petal_w</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_l  sepal_w  petal_l  petal_w         species\n",
       "0        5.1      3.5      1.4      0.2     Iris-setosa\n",
       "1        4.9      3.0      1.4      0.2     Iris-setosa\n",
       "2        4.7      3.2      1.3      0.2     Iris-setosa\n",
       "3        4.6      3.1      1.5      0.2     Iris-setosa\n",
       "4        5.0      3.6      1.4      0.2     Iris-setosa\n",
       "..       ...      ...      ...      ...             ...\n",
       "145      6.7      3.0      5.2      2.3  Iris-virginica\n",
       "146      6.3      2.5      5.0      1.9  Iris-virginica\n",
       "147      6.5      3.0      5.2      2.0  Iris-virginica\n",
       "148      6.2      3.4      5.4      2.3  Iris-virginica\n",
       "149      5.9      3.0      5.1      1.8  Iris-virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00206801 0.2718044  0.7261276 ]]\n",
      "Predict that [[5. 3. 4. 2.]] is: Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "## Some particular predictions for the arrays [5.0,3.0,4.0,2.0] and [5.0,3.0,4.0,2.0],[5.2,3.5,1.5,0.8].\n",
    "## Here we use our neural network.\n",
    "\n",
    "sample_flower = np.array( [[5.0,3.0,4.0,2.0]], dtype=float)\n",
    "pred = model.predict(sample_flower)\n",
    "print(pred)\n",
    "pred = np.argmax(pred)\n",
    "print(f\"Predict that {sample_flower} is: {species[pred]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00206801 0.2718043  0.7261277 ]\n",
      " [0.99772984 0.00227002 0.00000014]]\n",
      "Predict that these two flowers [[5.  3.  4.  2. ]\n",
      " [5.2 3.5 1.5 0.8]] \n",
      "are: Index(['Iris-virginica', 'Iris-setosa'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "sample_flower = np.array( [[5.0,3.0,4.0,2.0],[5.2,3.5,1.5,0.8]],\\\n",
    "        dtype=float)\n",
    "pred = model.predict(sample_flower)\n",
    "print(pred)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "print(f\"Predict that these two flowers {sample_flower} \")\n",
    "print(f\"are: {species[pred]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Keral Neural Networks\n",
    "\n",
    "\n",
    "We expend resources (time, computation time, money, etc) when training neural network. It is a good idea\n",
    "to save this neural networks so that they can be reloaded. (We save the weights).\n",
    "\n",
    "Keras provides 3 formas: YAML, JSON, HDF5.\n",
    "\n",
    "We will use HDF5.\n",
    "\n",
    "__Example:__ We use againg the GTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 - 0s - loss: 745.0228\n",
      "Epoch 2/100\n",
      "13/13 - 0s - loss: 165.3014\n",
      "Epoch 3/100\n",
      "13/13 - 0s - loss: 106.3204\n",
      "Epoch 4/100\n",
      "13/13 - 0s - loss: 101.2500\n",
      "Epoch 5/100\n",
      "13/13 - 0s - loss: 93.0149\n",
      "Epoch 6/100\n",
      "13/13 - 0s - loss: 87.9576\n",
      "Epoch 7/100\n",
      "13/13 - 0s - loss: 85.8098\n",
      "Epoch 8/100\n",
      "13/13 - 0s - loss: 86.2512\n",
      "Epoch 9/100\n",
      "13/13 - 0s - loss: 84.8327\n",
      "Epoch 10/100\n",
      "13/13 - 0s - loss: 86.8614\n",
      "Epoch 11/100\n",
      "13/13 - 0s - loss: 84.3174\n",
      "Epoch 12/100\n",
      "13/13 - 0s - loss: 81.0005\n",
      "Epoch 13/100\n",
      "13/13 - 0s - loss: 78.4610\n",
      "Epoch 14/100\n",
      "13/13 - 0s - loss: 79.5764\n",
      "Epoch 15/100\n",
      "13/13 - 0s - loss: 77.3994\n",
      "Epoch 16/100\n",
      "13/13 - 0s - loss: 76.6801\n",
      "Epoch 17/100\n",
      "13/13 - 0s - loss: 73.8983\n",
      "Epoch 18/100\n",
      "13/13 - 0s - loss: 73.4499\n",
      "Epoch 19/100\n",
      "13/13 - 0s - loss: 75.9567\n",
      "Epoch 20/100\n",
      "13/13 - 0s - loss: 77.2723\n",
      "Epoch 21/100\n",
      "13/13 - 0s - loss: 69.7043\n",
      "Epoch 22/100\n",
      "13/13 - 0s - loss: 69.4659\n",
      "Epoch 23/100\n",
      "13/13 - 0s - loss: 69.2528\n",
      "Epoch 24/100\n",
      "13/13 - 0s - loss: 73.5396\n",
      "Epoch 25/100\n",
      "13/13 - 0s - loss: 66.6785\n",
      "Epoch 26/100\n",
      "13/13 - 0s - loss: 78.9248\n",
      "Epoch 27/100\n",
      "13/13 - 0s - loss: 65.5918\n",
      "Epoch 28/100\n",
      "13/13 - 0s - loss: 64.6500\n",
      "Epoch 29/100\n",
      "13/13 - 0s - loss: 61.0418\n",
      "Epoch 30/100\n",
      "13/13 - 0s - loss: 60.9221\n",
      "Epoch 31/100\n",
      "13/13 - 0s - loss: 58.2359\n",
      "Epoch 32/100\n",
      "13/13 - 0s - loss: 62.2482\n",
      "Epoch 33/100\n",
      "13/13 - 0s - loss: 61.0155\n",
      "Epoch 34/100\n",
      "13/13 - 0s - loss: 57.9678\n",
      "Epoch 35/100\n",
      "13/13 - 0s - loss: 53.2597\n",
      "Epoch 36/100\n",
      "13/13 - 0s - loss: 53.7962\n",
      "Epoch 37/100\n",
      "13/13 - 0s - loss: 57.1762\n",
      "Epoch 38/100\n",
      "13/13 - 0s - loss: 52.1417\n",
      "Epoch 39/100\n",
      "13/13 - 0s - loss: 55.1704\n",
      "Epoch 40/100\n",
      "13/13 - 0s - loss: 48.2781\n",
      "Epoch 41/100\n",
      "13/13 - 0s - loss: 51.9604\n",
      "Epoch 42/100\n",
      "13/13 - 0s - loss: 49.2939\n",
      "Epoch 43/100\n",
      "13/13 - 0s - loss: 45.7989\n",
      "Epoch 44/100\n",
      "13/13 - 0s - loss: 44.4633\n",
      "Epoch 45/100\n",
      "13/13 - 0s - loss: 48.8320\n",
      "Epoch 46/100\n",
      "13/13 - 0s - loss: 47.1311\n",
      "Epoch 47/100\n",
      "13/13 - 0s - loss: 41.3886\n",
      "Epoch 48/100\n",
      "13/13 - 0s - loss: 43.2518\n",
      "Epoch 49/100\n",
      "13/13 - 0s - loss: 46.0600\n",
      "Epoch 50/100\n",
      "13/13 - 0s - loss: 39.6368\n",
      "Epoch 51/100\n",
      "13/13 - 0s - loss: 37.9085\n",
      "Epoch 52/100\n",
      "13/13 - 0s - loss: 37.5028\n",
      "Epoch 53/100\n",
      "13/13 - 0s - loss: 37.6385\n",
      "Epoch 54/100\n",
      "13/13 - 0s - loss: 36.3472\n",
      "Epoch 55/100\n",
      "13/13 - 0s - loss: 37.9264\n",
      "Epoch 56/100\n",
      "13/13 - 0s - loss: 37.5132\n",
      "Epoch 57/100\n",
      "13/13 - 0s - loss: 41.8614\n",
      "Epoch 58/100\n",
      "13/13 - 0s - loss: 40.9528\n",
      "Epoch 59/100\n",
      "13/13 - 0s - loss: 37.9255\n",
      "Epoch 60/100\n",
      "13/13 - 0s - loss: 36.4361\n",
      "Epoch 61/100\n",
      "13/13 - 0s - loss: 32.3542\n",
      "Epoch 62/100\n",
      "13/13 - 0s - loss: 32.2918\n",
      "Epoch 63/100\n",
      "13/13 - 0s - loss: 31.1731\n",
      "Epoch 64/100\n",
      "13/13 - 0s - loss: 32.7470\n",
      "Epoch 65/100\n",
      "13/13 - 0s - loss: 33.0293\n",
      "Epoch 66/100\n",
      "13/13 - 0s - loss: 33.6866\n",
      "Epoch 67/100\n",
      "13/13 - 0s - loss: 31.6542\n",
      "Epoch 68/100\n",
      "13/13 - 0s - loss: 29.7316\n",
      "Epoch 69/100\n",
      "13/13 - 0s - loss: 28.5483\n",
      "Epoch 70/100\n",
      "13/13 - 0s - loss: 27.2425\n",
      "Epoch 71/100\n",
      "13/13 - 0s - loss: 31.8351\n",
      "Epoch 72/100\n",
      "13/13 - 0s - loss: 32.6672\n",
      "Epoch 73/100\n",
      "13/13 - 0s - loss: 29.1398\n",
      "Epoch 74/100\n",
      "13/13 - 0s - loss: 25.4417\n",
      "Epoch 75/100\n",
      "13/13 - 0s - loss: 26.1743\n",
      "Epoch 76/100\n",
      "13/13 - 0s - loss: 29.2864\n",
      "Epoch 77/100\n",
      "13/13 - 0s - loss: 24.8836\n",
      "Epoch 78/100\n",
      "13/13 - 0s - loss: 25.1118\n",
      "Epoch 79/100\n",
      "13/13 - 0s - loss: 23.4652\n",
      "Epoch 80/100\n",
      "13/13 - 0s - loss: 27.3546\n",
      "Epoch 81/100\n",
      "13/13 - 0s - loss: 23.7232\n",
      "Epoch 82/100\n",
      "13/13 - 0s - loss: 23.0423\n",
      "Epoch 83/100\n",
      "13/13 - 0s - loss: 23.6897\n",
      "Epoch 84/100\n",
      "13/13 - 0s - loss: 24.6037\n",
      "Epoch 85/100\n",
      "13/13 - 0s - loss: 21.9940\n",
      "Epoch 86/100\n",
      "13/13 - 0s - loss: 22.1688\n",
      "Epoch 87/100\n",
      "13/13 - 0s - loss: 22.5557\n",
      "Epoch 88/100\n",
      "13/13 - 0s - loss: 23.1813\n",
      "Epoch 89/100\n",
      "13/13 - 0s - loss: 21.2260\n",
      "Epoch 90/100\n",
      "13/13 - 0s - loss: 21.5593\n",
      "Epoch 91/100\n",
      "13/13 - 0s - loss: 21.0154\n",
      "Epoch 92/100\n",
      "13/13 - 0s - loss: 21.2989\n",
      "Epoch 93/100\n",
      "13/13 - 0s - loss: 21.7614\n",
      "Epoch 94/100\n",
      "13/13 - 0s - loss: 20.1338\n",
      "Epoch 95/100\n",
      "13/13 - 0s - loss: 21.3780\n",
      "Epoch 96/100\n",
      "13/13 - 0s - loss: 19.5566\n",
      "Epoch 97/100\n",
      "13/13 - 0s - loss: 20.5347\n",
      "Epoch 98/100\n",
      "13/13 - 0s - loss: 19.9564\n",
      "Epoch 99/100\n",
      "13/13 - 0s - loss: 18.6864\n",
      "Epoch 100/100\n",
      "13/13 - 0s - loss: 19.1637\n",
      "Before save score (RMSE): 4.443067684005931\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "save_path = \".\" # we save the neural network in the same folder\n",
    "\n",
    "# Loading the data set from Heaton's website.\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\", \n",
    "    na_values=['NA', '?'])\n",
    "\n",
    "cars = df['name']\n",
    "\n",
    "# Filling the empty values\n",
    "\n",
    "\n",
    "df['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())\n",
    "\n",
    "# Obtaining a matrix from a data set \n",
    "\n",
    "x = df[['cylinders', 'displacement', 'horsepower', 'weight',\n",
    "       'acceleration', 'year', 'origin']].values # this is the predictor\n",
    "y = df['mpg'].values # regression --> this is the dependent variable\n",
    "\n",
    "# Build the neural network -> 4 layers, inputs, hidden layer, hidden layer, output\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=x.shape[1], activation='relu')) # Hidden 1, 25 neurons after the inputs\n",
    "model.add(Dense(10, activation='relu')) # Hidden 2, 10 neurons \n",
    "model.add(Dense(1)) # Output, 1 neuron for the output\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(x,y,verbose=2,epochs=100)\n",
    "\n",
    "# Predict\n",
    "pred = model.predict(x)\n",
    "\n",
    "# Mean Square Error\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y))\n",
    "print(f\"Before save score (RMSE): {score}\")\n",
    "\n",
    "# Some remarks about saving a neural network.\n",
    "\n",
    "# When we train a neural network we are minimizing the error for the weights w\n",
    "# This happens to be equivalent to some Generalized linear model where you \n",
    "# try to fit some data set using a particular model which uses weights.\n",
    "# One example of these models is logistic regression, where given a matrix\n",
    "# X containing the data set you want to find the probability for a certain\n",
    "# row for belonging to certain group or category. \n",
    "\n",
    "\n",
    "# save neural network structure to JSON (no weights)\n",
    "model_json = model.to_json() # this is the code for saving the neuron\n",
    "with open(os.path.join(save_path,\"network.json\"), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# save neural network structure to YAML (no weights)\n",
    "model_yaml = model.to_yaml()\n",
    "with open(os.path.join(save_path,\"network.yaml\"), \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "\n",
    "# save entire network to HDF5 (save everything, suggested)\n",
    "model.save(os.path.join(save_path,\"network.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reloading a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
