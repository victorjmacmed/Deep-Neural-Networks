{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Regularization and dropout, K-fold cross validation\n",
    "## 2. L1 and L2 regularization to decrease overfitting\n",
    "## 3. Drop out to decrease overfitting\n",
    "## 4. Benchmarking Keras Deep Learning regularization Techniques \n",
    "\n",
    "## All with keras\n",
    "\n",
    "These are some modified, I added some comments and examples, and summarize notes from a course in Deep Neural Networks which took place at Washington University in St. Louis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Regularization: Ridge and Lasso\n",
    "\n",
    "Regularization is a technique that reduces overfitting, which occurs when neural networks attempt to memorize training data, rather than learn from it. Humans are capable of overfitting as well. Think about studying for one exam using old exam models. You will end memorizing the exam solutions and not, necesarilly, the techniques which are necessary to solve the problems.\n",
    "\n",
    "Although a neural network received a high score on its training data, this result does not mean that the same neural network will score high on data that was not inside the training set. Regularization is one of the techniques that can prevent overfitting. A number of different regularization techniques exist. Most work by analyzing and potentially modifying the weights of a neural network as it trains.\n",
    "\n",
    "\n",
    "## L1 and L2 regularization\n",
    "\n",
    "L1 and L2 regularization are two common regularization techniques that can reduce the effects of overfitting.\n",
    "\n",
    "Both of these algorithms work by adding a weight penalty to the neural network training. This penalty encourages the neural network to keep the weights to small values. Both L1 and L2 calculate this penalty differently. For gradient-descent-based algorithms, such as backpropagation, you can add this penalty calculation to the calculated gradients. For objective-function-based training, such as simulated annealing, the penalty is negatively combined with the objective score.\n",
    "\n",
    "__Example:__ We are going to look at linear regression to see how L1 and L2 regularization work. The following code sets up the auto-mpg data for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\", \n",
    "    na_values=['NA', '?']) # loading the dataset \n",
    "\n",
    "# Handle missing value\n",
    "df['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())\n",
    "\n",
    "# Pandas to Numpy\n",
    "names = ['cylinders', 'displacement', 'horsepower', 'weight',\n",
    "       'acceleration', 'year', 'origin']\n",
    "x = df[names].values # predictors\n",
    "y = df['mpg'].values # dependent variable\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=45) # using the split function to split the data into training and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to evaluate the coefficients of a regression\n",
    "\n",
    "%matplotlib inline    \n",
    "from IPython.display import display, HTML    \n",
    "\n",
    "def report_coef(names,coef,intercept):\n",
    "    r = pd.DataFrame( { 'coef': coef, 'positive': coef>=0  }, index = names )\n",
    "    r = r.sort_values(by=['coef'])\n",
    "    display(r)\n",
    "    print(f\"Intercept: {intercept}\")\n",
    "    r['coef'].plot(kind='barh', color=r['positive'].map(\n",
    "        {True: 'b', False: 'r'}))\n",
    "\n",
    "# Explanation: (this is a little bit ad hoc here) We pass as arguments 'names', coefficients of regression and \n",
    "# the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "L1/L2 regularization applied to linear regression. \n",
    "\n",
    "__Example:__ Linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 3.0019345985860784\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cylinders</th>\n",
       "      <td>-0.427721</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>-0.007255</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horsepower</th>\n",
       "      <td>-0.005491</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>displacement</th>\n",
       "      <td>0.020166</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acceleration</th>\n",
       "      <td>0.138575</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.783047</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>origin</th>\n",
       "      <td>1.003762</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  coef  positive\n",
       "cylinders    -0.427721     False\n",
       "weight       -0.007255     False\n",
       "horsepower   -0.005491     False\n",
       "displacement  0.020166      True\n",
       "acceleration  0.138575      True\n",
       "year          0.783047      True\n",
       "origin        1.003762      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: -19.101231042200084\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "# Create linear regression\n",
    "regressor = sklearn.linear_model.LinearRegression() \n",
    "\n",
    "# Fit/train linear regression\n",
    "regressor.fit(x_train,y_train)\n",
    "\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(f\"Final score (RMSE): {score}\")\n",
    "\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_,\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 (Lasso) Regularization\n",
    "\n",
    "L1 is implemented by adding the following error to the objective to minimize:\n",
    "\n",
    "$ E_1 = \\alpha \\sum_w{ |w| } $\n",
    "\n",
    "This is the $l^1$ norm multiplied by a constant (or 1-norm because the sum is finite) of the weights.\n",
    "\n",
    "We add $E_1$ to the error of the regression or method, then the algorithm should keep working to minimize\n",
    "the weights. \n",
    "\n",
    "$$Loss = Error(y,\\hat{y}) + \\alpha \\sum_{i=1}^{N}|w_i|$$\n",
    "\n",
    "You should use L1 regularization to create sparsity in the neural network. In other words, the L1 algorithm will push many weight connections to near 0.  When a weight is near 0, the program drops it from the network.  Dropping weighted connections will create a sparse neural network.\n",
    "Feature selection is a useful byproduct of sparse neural networks. Features are the values that the training set provides to the input neurons.  Once all the weights of an input neuron reach 0, the neural network training determines that the feature is unnecessary.  If your data set has a large number of input features that may not be needed, L1 regularization can help the neural network detect and ignore unnecessary features.\n",
    "\n",
    "The following code demonstrates lasso regression.  Notice the effect of the coefficients compared to the previous section that used linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 3.0604021904033303\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cylinders</th>\n",
       "      <td>-0.012995</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>-0.007328</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horsepower</th>\n",
       "      <td>-0.002715</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>displacement</th>\n",
       "      <td>0.011601</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acceleration</th>\n",
       "      <td>0.114391</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>origin</th>\n",
       "      <td>0.708222</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.777480</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  coef  positive\n",
       "cylinders    -0.012995     False\n",
       "weight       -0.007328     False\n",
       "horsepower   -0.002715     False\n",
       "displacement  0.011601      True\n",
       "acceleration  0.114391      True\n",
       "origin        0.708222      True\n",
       "year          0.777480      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: -18.506677982383223\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Create linear regression\n",
    "regressor = Lasso(random_state=0,alpha=0.1) # alpha is the parameter in the loss\n",
    "\n",
    "# Fit/train LASSO\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(f\"Final score (RMSE): {score}\")\n",
    "\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_,\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1e-08, 100000000.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lasso = Lasso(random_state=42) \n",
    "alphas = np.logspace(-8, 8, 10) #logspace -> Return numbers spaced evenly on a log scale.\n",
    "\n",
    "\n",
    "scores = list() # empty list - recall that list are not modifiable \n",
    "scores_std = list()\n",
    "\n",
    "n_folds = 3 # This the parameter for the cross-validation\n",
    "\n",
    "# cross_val_score -> Evaluate a score by cross-validation\n",
    "# We use cross-validation to test the model. \n",
    "\n",
    "# This piece of code takes logspaced values\n",
    "# then \n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso.alpha = alpha # we change alpha in the loss function \n",
    "    this_scores = cross_val_score(lasso, x, y, cv=n_folds, n_jobs=1) # we perform cross-validation\n",
    "    scores.append(np.mean(this_scores)) # we add the mean to a list\n",
    "    scores_std.append(np.std(this_scores)) # we add the standard deviation to the list\n",
    "\n",
    "scores, scores_std = np.array(scores), np.array(scores_std) # we turn these variables into arrays\n",
    "\n",
    "plt.figure().set_size_inches(8, 6) \n",
    "plt.semilogx(alphas, scores) # semilogx changes the x axis to log scaling\n",
    "\n",
    "# plot error lines showing +/- std. errors of the scores\n",
    "std_error = scores_std / np.sqrt(n_folds)\n",
    "\n",
    "plt.semilogx(alphas, scores + std_error, 'b--') # The boundary function above the region\n",
    "plt.semilogx(alphas, scores - std_error, 'b--') # The boundary function below the region\n",
    "\n",
    "# alpha=0.2 controls the translucency of the fill color\n",
    "plt.fill_between(alphas, scores + std_error, scores - std_error, alpha=0.2) # this fill the space between the functions\n",
    "\n",
    "plt.ylabel('CV score +/- std error')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.max(scores), linestyle='--', color='.5')\n",
    "plt.xlim([alphas[0], alphas[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 regularization\n",
    "\n",
    "Here we use the $l^2$ norm of the weights to write the loss function.\n",
    "\n",
    "$$E_2 = \\alpha \\sum_{\\omega}\\omega^2$$\n",
    "\n",
    "We have \n",
    "\n",
    "$$Loss = Error(y,\\hat{y}) + \\alpha \\sum_{i=1}^{N}|w_i|^2$$\n",
    "\n",
    "Like the L1 algorithm, the $\\alpha$ value determines how important the L2 objective is compared to the neural networks error.  Typical L2 values are below 0.1 (10%).  The main calculation performed by L2 is the summing of the squares of all of the weights.  The bias values are not summed.\n",
    "\n",
    "You should use L2 regularization when you are less concerned about creating a space network and are more concerned about low weight values.  The lower weight values will typically lead to less overfitting.  Generally L2 regularization will produce better overall performance than L1.  However, L1 might be useful in situations where there are a large number of inputs and some of the weaker inputs should be pruned.\n",
    "\n",
    "The following code uses L2 with linear regression (Ridge regression):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): {score}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cylinders</th>\n",
       "      <td>-0.421393</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>-0.007257</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horsepower</th>\n",
       "      <td>-0.005385</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>displacement</th>\n",
       "      <td>0.020006</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acceleration</th>\n",
       "      <td>0.138470</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.782889</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>origin</th>\n",
       "      <td>0.994621</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  coef  positive\n",
       "cylinders    -0.421393     False\n",
       "weight       -0.007257     False\n",
       "horsepower   -0.005385     False\n",
       "displacement  0.020006      True\n",
       "acceleration  0.138470      True\n",
       "year          0.782889      True\n",
       "origin        0.994621      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: -19.079800744254825\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Create linear regression\n",
    "regressor = Ridge(alpha=1) # Ridge give us Linear least squares with l2 regularization.\n",
    "\n",
    "# Fit/train Ridge\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {score}\")\n",
    "\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_,\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticNet Regularization\n",
    "\n",
    "The ElasticNet regression combines both L1 and L2.  Both penalties are applied.  The amount of L1 and L2 are governed by the parameters alpha and beta.\n",
    "\n",
    "$ a \\cdot L1 + b \\cdot L2 $\n",
    "\n",
    "In this case the model should minimize the function\n",
    "\n",
    "$$Loss = Error(y,\\hat{y}) + \\lambda_1 \\sum_{j=1}^{N}|w_j|^2 + \\lambda_2\\sum_{i=1}^{N}|w_i|$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 3.0450899960775026\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cylinders</th>\n",
       "      <td>-0.274010</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>-0.007303</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horsepower</th>\n",
       "      <td>-0.003231</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>displacement</th>\n",
       "      <td>0.016194</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acceleration</th>\n",
       "      <td>0.132348</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.777482</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>origin</th>\n",
       "      <td>0.782781</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  coef  positive\n",
       "cylinders    -0.274010     False\n",
       "weight       -0.007303     False\n",
       "horsepower   -0.003231     False\n",
       "displacement  0.016194      True\n",
       "acceleration  0.132348      True\n",
       "year          0.777482      True\n",
       "origin        0.782781      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: -18.389355690429745\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Create linear regression\n",
    "regressor = ElasticNet(alpha=0.1, l1_ratio=0.1) # Elastic regularization\n",
    "\n",
    "# Fit/train LASSO\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(f\"Final score (RMSE): {score}\")\n",
    "\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_,\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold cross validation\n",
    "\n",
    "Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. \n",
    "The procedure has a single parameter called k that refers to the number of groups that a given data sample \n",
    "is to be split into.\n",
    "\n",
    "![title](cross.png)\n",
    "\n",
    "Credit: Jeff Heaton\n",
    "\n",
    "Important ideas:\n",
    "\n",
    "1. There will be a neural network for each fold\n",
    "2. Each neural network will one model of our complete data set.\n",
    "3. We want to generate predictions for data that is not present in the data set.\n",
    "\n",
    "It is important to note that there will be one model (neural network) for each fold. To generate predictions for new data, which is data not present in the training set, predictions from the fold models can be handled in several ways:\n",
    "\n",
    "* Choose the model that had the highest validation score as the final model.\n",
    "* Preset new data to the 5 models (one for each fold) and average the result (this is an [ensemble](https://en.wikipedia.org/wiki/Ensemble_learning)).\n",
    "* Retrain a new model (using the same settings as the cross-validation) on the entire dataset.  Train for as many epochs, and with the same hidden layer structure.\n",
    "\n",
    "Regression and classification are handled somewhat differently with regards to cross-validation.\n",
    "\n",
    "1. Regression -> simple case -> we follow the picture above -> randomly assign k-folds\n",
    "2. Classification -> more complex case -> when you divide the data set you should be careful, \n",
    "   we should have a 'uniform' number of elements in each class into these k-folds. This is a problem.\n",
    "3. To solve this problem we use two different methods.\n",
    "\n",
    "* **KFold** When dealing with a regression problem.\n",
    "* **StratifiedKFold** When dealing with a classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code trains the simple dataset using a 5-fold cross-validation.  The expected performance of a neural network, of the type trained here, would be the score for the generated out-of-sample predictions.  We begin by preparing a feature vector using the jh-simple-dataset to predict age.  This is a regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data set\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
    "    na_values=['NA','?']) # loading the dataset from Jeff Heaton's website\n",
    "\n",
    "# Generate dummies for job\n",
    "df = pd.concat([df,pd.get_dummies(df['job'],prefix=\"job\")],axis=1) # create dummy variables using the categorical variable job\n",
    "df.drop('job', axis=1, inplace=True) # drop original job column\n",
    "\n",
    "# Generate dummies for area -> same process than the job variable\n",
    "df = pd.concat([df,pd.get_dummies(df['area'],prefix=\"area\")],axis=1)\n",
    "df.drop('area', axis=1, inplace=True)\n",
    "\n",
    "# Generate dummies for product\n",
    "df = pd.concat([df,pd.get_dummies(df['product'],prefix=\"product\")],axis=1)\n",
    "df.drop('product', axis=1, inplace=True)\n",
    "\n",
    "# Missing values for income\n",
    "med = df['income'].median()\n",
    "df['income'] = df['income'].fillna(med) # filling with the median \n",
    "\n",
    "# Standardize ranges\n",
    "df['income'] = zscore(df['income'])\n",
    "df['aspect'] = zscore(df['aspect'])\n",
    "df['save_rate'] = zscore(df['save_rate'])\n",
    "df['subscriptions'] = zscore(df['subscriptions'])\n",
    "\n",
    "# Convert to numpy - Classification\n",
    "x_columns = df.columns.drop('age').drop('id')\n",
    "x = df[x_columns].values\n",
    "y = df['age'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the feature vector is created a 5-fold cross-validation can be performed to generate out of sample \n",
    "predictions.  We will assume 500 epochs, and not use early stopping.  Later we will see how we can estimate \n",
    "a more optimal epoch count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "Fold score (RMSE): 0.6563854674738708\n",
      "Fold #2\n",
      "Fold score (RMSE): 0.5076153664490587\n",
      "Fold #3\n",
      "Fold score (RMSE): 0.6049024375497042\n",
      "Fold #4\n",
      "Fold score (RMSE): 0.5026600874974161\n",
      "Fold #5\n",
      "Fold score (RMSE): 0.9334523809286515\n",
      "Final, out of sample score (RMSE): 0.6600640441872222\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "# Cross-Validate\n",
    "kf = KFold(5, shuffle=True, random_state=42) # Use for KFold classification, k=5\n",
    "    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "\n",
    "fold = 0\n",
    "for train, test in kf.split(x): # taking different subsets\n",
    "    fold+=1\n",
    "    print(f\"Fold #{fold}\")\n",
    "        \n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=x.shape[1], activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),verbose=0,\n",
    "              epochs=500) # epochs are iterations using the data set to train the network\n",
    "    \n",
    "    pred = model.predict(x_test)\n",
    "    \n",
    "    oos_y.append(y_test)\n",
    "    oos_pred.append(pred)    \n",
    "\n",
    "    # Measure this fold's RMSE\n",
    "    score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "    print(f\"Fold score (RMSE): {score}\")\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_pred,oos_y))\n",
    "print(f\"Final, out of sample score (RMSE): {score}\")    \n",
    "    \n",
    "# Write the cross-validated prediction\n",
    "oos_y = pd.DataFrame(oos_y)\n",
    "oos_pred = pd.DataFrame(oos_pred)\n",
    "oosDF = pd.concat( [df, oos_y, oos_pred],axis=1 )\n",
    "#oosDF.to_csv(filename_write,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the above code also reports the average number of epochs needed.  A common technique is to then train on the entire dataset for the average number of epochs needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification with Stratified K-Fold Cross-Validation\n",
    "\n",
    "The following code trains and fits the jh-simple-dataset dataset with cross-validation to generate out-of-sample . It also writes out the out of sample (predictions on the test set) results.\n",
    "It is good to perform a stratified k-fold cross validation with classification data. This ensures that the percentages of each class remains the same across all folds. To do this, make use of the StratifiedKFold object, instead of the KFold object used in regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating another dataset\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Read the data set\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Generate dummies for job\n",
    "df = pd.concat([df,pd.get_dummies(df['job'],prefix=\"job\")],axis=1)\n",
    "df.drop('job', axis=1, inplace=True)\n",
    "\n",
    "# Generate dummies for area\n",
    "df = pd.concat([df,pd.get_dummies(df['area'],prefix=\"area\")],axis=1)\n",
    "df.drop('area', axis=1, inplace=True)\n",
    "\n",
    "# Missing values for income\n",
    "med = df['income'].median()\n",
    "df['income'] = df['income'].fillna(med)\n",
    "\n",
    "# Standardize ranges\n",
    "df['income'] = zscore(df['income'])\n",
    "df['aspect'] = zscore(df['aspect'])\n",
    "df['save_rate'] = zscore(df['save_rate'])\n",
    "df['age'] = zscore(df['age'])\n",
    "df['subscriptions'] = zscore(df['subscriptions'])\n",
    "\n",
    "# Convert to numpy - Classification\n",
    "x_columns = df.columns.drop('product').drop('id')\n",
    "x = df[x_columns].values\n",
    "dummies = pd.get_dummies(df['product']) # Classification\n",
    "products = dummies.columns\n",
    "y = dummies.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume 500 epochs, and not use early stopping.  \n",
    "Later we will see how we can estimate a more optimal epoch count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "Fold score (accuracy): 0.66\n",
      "Fold #2\n",
      "Fold score (accuracy): 0.69\n",
      "Fold #3\n",
      "Fold score (accuracy): 0.6825\n",
      "Fold #4\n",
      "Fold score (accuracy): 0.665\n",
      "Fold #5\n",
      "Fold score (accuracy): 0.695\n",
      "Final score (accuracy): 0.6785\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "# np.argmax(pred,axis=1)\n",
    "# Cross-validate\n",
    "# Use for StratifiedKFold classification\n",
    "kf = StratifiedKFold(5, shuffle=True, random_state=42) \n",
    "    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "# Must specify y StratifiedKFold for\n",
    "for train, test in kf.split(x,df['product']):  \n",
    "    fold+=1\n",
    "    print(f\"Fold #{fold}\")\n",
    "        \n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
    "    model.add(Dense(25, activation='relu')) # Hidden 2\n",
    "    model.add(Dense(y.shape[1],activation='softmax')) # Output\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),verbose=0,\\\n",
    "              epochs=500)\n",
    "    \n",
    "    pred = model.predict(x_test)\n",
    "    \n",
    "    oos_y.append(y_test)\n",
    "    # raw probabilities to chosen class (highest probability)\n",
    "    pred = np.argmax(pred,axis=1) \n",
    "    oos_pred.append(pred)  \n",
    "\n",
    "    # Measure this fold's accuracy\n",
    "    y_compare = np.argmax(y_test,axis=1) # For accuracy calculation\n",
    "    score = metrics.accuracy_score(y_compare, pred)\n",
    "    print(f\"Fold score (accuracy): {score}\")\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "oos_y_compare = np.argmax(oos_y,axis=1) # For accuracy calculation\n",
    "\n",
    "score = metrics.accuracy_score(oos_y_compare, oos_pred)\n",
    "print(f\"Final score (accuracy): {score}\")    \n",
    "    \n",
    "# Write the cross-validated prediction\n",
    "oos_y = pd.DataFrame(oos_y)\n",
    "oos_pred = pd.DataFrame(oos_pred)\n",
    "oosDF = pd.concat( [df, oos_y, oos_pred],axis=1 )\n",
    "#oosDF.to_csv(filename_write,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with both a Cross-Validation and a Holdout Set\n",
    "\n",
    "If you have a considerable amount of data, it is always valuable to set aside a holdout set before you cross-validate.  This hold out set will be the final evaluation before you make use of your model for its real-world use. Figure 5.HOLDOUT shows this division.\n",
    "\n",
    "**Figure 5.HOLDOUT: Cross Validation and a Holdout Set**\n",
    "![title](class_3_hold_train_val.png)\n",
    "\n",
    "Credit: Jeff Heaton\n",
    "\n",
    "The following program makes use of a holdout set, and then still cross-validates.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data set\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Generate dummies for job\n",
    "df = pd.concat([df,pd.get_dummies(df['job'],prefix=\"job\")],axis=1)\n",
    "df.drop('job', axis=1, inplace=True)\n",
    "\n",
    "# Generate dummies for area\n",
    "df = pd.concat([df,pd.get_dummies(df['area'],prefix=\"area\")],axis=1)\n",
    "df.drop('area', axis=1, inplace=True)\n",
    "\n",
    "# Generate dummies for product\n",
    "df = pd.concat([df,pd.get_dummies(df['product'],prefix=\"product\")],axis=1)\n",
    "df.drop('product', axis=1, inplace=True)\n",
    "\n",
    "# Missing values for income\n",
    "med = df['income'].median()\n",
    "df['income'] = df['income'].fillna(med)\n",
    "\n",
    "# Standardize ranges\n",
    "df['income'] = zscore(df['income'])\n",
    "df['aspect'] = zscore(df['aspect'])\n",
    "df['save_rate'] = zscore(df['save_rate'])\n",
    "df['subscriptions'] = zscore(df['subscriptions'])\n",
    "\n",
    "# Convert to numpy - Classification\n",
    "x_columns = df.columns.drop('age').drop('id')\n",
    "x = df[x_columns].values\n",
    "y = df['age'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been preprocessed, we are ready to build the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "Fold score (RMSE): 0.6120134746621881\n",
      "Fold #2\n",
      "Fold score (RMSE): 0.5214205090453491\n",
      "Fold #3\n",
      "Fold score (RMSE): 0.914298759700343\n",
      "Fold #4\n",
      "Fold score (RMSE): 0.8591843094630229\n",
      "Fold #5\n",
      "Fold score (RMSE): 0.5101832537409827\n",
      "\n",
      "Cross-validated score (RMSE): 0.704395725772945\n",
      "Holdout score (RMSE): 0.7414697804215943\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Keep a 10% holdout\n",
    "x_main, x_holdout, y_main, y_holdout = train_test_split(    \n",
    "    x, y, test_size=0.10) \n",
    "\n",
    "\n",
    "# Cross-validate\n",
    "kf = KFold(5)\n",
    "    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "for train, test in kf.split(x_main):        \n",
    "    fold+=1\n",
    "    print(f\"Fold #{fold}\")\n",
    "        \n",
    "    x_train = x_main[train]\n",
    "    y_train = y_main[train]\n",
    "    x_test = x_main[test]\n",
    "    y_test = y_main[test]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=x.shape[1], activation='relu'))\n",
    "    model.add(Dense(5, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),\n",
    "              verbose=0,epochs=500)\n",
    "    \n",
    "    pred = model.predict(x_test)\n",
    "    \n",
    "    oos_y.append(y_test)\n",
    "    oos_pred.append(pred) \n",
    "\n",
    "    # Measure accuracy\n",
    "    score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "    print(f\"Fold score (RMSE): {score}\")\n",
    "\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_pred,oos_y))\n",
    "print()\n",
    "print(f\"Cross-validated score (RMSE): {score}\")    \n",
    "    \n",
    "# Write the cross-validated prediction (from the last neural network)\n",
    "holdout_pred = model.predict(x_holdout)\n",
    "\n",
    "score = np.sqrt(metrics.mean_squared_error(holdout_pred,y_holdout))\n",
    "print(f\"Holdout score (RMSE): {score}\")  \n",
    "\n",
    "# This is computationally hard "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 and L2 regularization to Decrease Overfitting\n",
    "\n",
    "L1 and L2 regularization are two common regularization techniques that can reduce the effects of overfitting. \n",
    "\n",
    "Both of these algorithms work by adding a weight penalty to the neural network training. This penalty encourages the neural network to keep the weights to small values. Both L1 and L2 calculate this penalty differently. For gradient-descent-based algorithms, such as backpropagation, you can add this penalty calculation to the calculated gradients. For objective-function-based training, such as simulated annealing, the penalty is negatively combined with the objective score.\n",
    "\n",
    "Both L1 and L2 work differently in the way that they penalize the size of a weight. L2 will force the weights into a pattern similar to a Gaussian distribution; the L1 will force the weights into a pattern similar to a Laplace distribution, as demonstrated in Figure 5.L1L2.\n",
    "\n",
    "![title](L1L2.png)\n",
    "\n",
    "credit: Jeff Heaton\n",
    "\n",
    "As you can see, L1 algorithm is more tolerant of weights further from 0, whereas the L2 algorithm is less tolerant. We will highlight other important differences between L1 and L2 in the following sections. You also need to note that both L1 and L2 count their penalties based only on weights; they do not count penalties on bias values. Keras allows l1/l2 to be directly added to your network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the dataset\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Read the data set\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Generate dummies for job\n",
    "df = pd.concat([df,pd.get_dummies(df['job'],prefix=\"job\")],axis=1)\n",
    "df.drop('job', axis=1, inplace=True)\n",
    "\n",
    "# Generate dummies for area\n",
    "df = pd.concat([df,pd.get_dummies(df['area'],prefix=\"area\")],axis=1)\n",
    "df.drop('area', axis=1, inplace=True)\n",
    "\n",
    "# Missing values for income\n",
    "med = df['income'].median()\n",
    "df['income'] = df['income'].fillna(med)\n",
    "\n",
    "# Standardize ranges\n",
    "df['income'] = zscore(df['income'])\n",
    "df['aspect'] = zscore(df['aspect'])\n",
    "df['save_rate'] = zscore(df['save_rate'])\n",
    "df['age'] = zscore(df['age'])\n",
    "df['subscriptions'] = zscore(df['subscriptions'])\n",
    "\n",
    "# Convert to numpy - Classification\n",
    "x_columns = df.columns.drop('product').drop('id')\n",
    "x = df[x_columns].values\n",
    "dummies = pd.get_dummies(df['product']) # Classification\n",
    "products = dummies.columns\n",
    "y = dummies.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Keras with L1/L2 for Regression\n",
    "########################################\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Cross-validate\n",
    "kf = KFold(5, shuffle=True, random_state=42)\n",
    "    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "for train, test in kf.split(x):\n",
    "    fold+=1\n",
    "    print(f\"Fold #{fold}\")\n",
    "        \n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    #kernel_regularizer=regularizers.l2(0.01),\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=x.shape[1], \n",
    "            activation='relu',\n",
    "             activity_regularizer=regularizers.l1(1e-4))) # Hidden 1\n",
    "    model.add(Dense(25, activation='relu', \n",
    "                    activity_regularizer=regularizers.l1(1e-4))) # Hidden 2\n",
    "    model.add(Dense(y.shape[1],activation='softmax')) # Output\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),\n",
    "              verbose=0,epochs=500)\n",
    "    \n",
    "    pred = model.predict(x_test)\n",
    "    \n",
    "    oos_y.append(y_test)\n",
    "    # raw probabilities to chosen class (highest probability)\n",
    "    pred = np.argmax(pred,axis=1) \n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure this fold's accuracy\n",
    "    y_compare = np.argmax(y_test,axis=1) # For accuracy calculation\n",
    "    score = metrics.accuracy_score(y_compare, pred)\n",
    "    print(f\"Fold score (accuracy): {score}\")\n",
    "\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "oos_y_compare = np.argmax(oos_y,axis=1) # For accuracy calculation\n",
    "\n",
    "score = metrics.accuracy_score(oos_y_compare, oos_pred)\n",
    "print(f\"Final score (accuracy): {score}\")    \n",
    "    \n",
    "# Write the cross-validated prediction\n",
    "oos_y = pd.DataFrame(oos_y)\n",
    "oos_pred = pd.DataFrame(oos_pred)\n",
    "oosDF = pd.concat( [df, oos_y, oos_pred],axis=1 )\n",
    "#oosDF.to_csv(filename_write,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Out for Keras to Decrease Overfitting\n",
    "\n",
    "__Key idea:__ You train the network dropping some neurons. This way you avoid that groups of neurons \n",
    "'working together', and each neuron in the neural network is 'well trained' 'independently in groups' of the other\n",
    "neurons.\n",
    "\n",
    "The key idea is on this link. https://yusugomori.com/projects/deep-learning/dropout-relu\n",
    "\n",
    "\n",
    "\n",
    "Hinton, Srivastava, Krizhevsky, Sutskever, & Salakhutdinov (2012) introduced the dropout regularization algorithm. [Cite:srivastava2014dropout] Although dropout works in a different way than L1 and L2, it accomplishes the same goalthe prevention of overfitting. However, the algorithm goes about the task by actually removing neurons and connectionsat least temporarily. Unlike L1 and L2, no weight penalty is added. Dropout does not directly seek to train small weights. Dropout works by causing hidden neurons of the neural network to be unavailable during part of the training. Dropping part of the neural network causes the remaining portion to be trained to still achieve a good score even without the dropped neurons. This decreases coadaption between neurons, which results in less overfitting.\n",
    "Most neural network frameworks implement dropout as a separate layer. Dropout layers function as a regular, densely connected neural network layer. The only difference is that the dropout layers will periodically drop some of their neurons during training. You can use dropout layers on regular feedforward neural networks.\n",
    "The program implements a dropout layer as a dense layer that can eliminate some of its neurons. Contrary to popular belief about the dropout layer, the program does not permanently remove these discarded neurons. A dropout layer does not lose any of its neurons during the training process, and it will still have exactly the same number of neurons after training. In this way, the program only temporarily masks the neurons rather than dropping them. The figure below shows how a dropout layer might be situated with other layers.\n",
    "\n",
    "![title](dropout.png)\n",
    "\n",
    "Credit: Jeff Heaton\n",
    "\n",
    "The discarded neurons and their connections are shown as dashed lines. The input layer has two input neurons as well as a bias neuron. The second layer is a dense layer with three neurons as well as a bias neuron. The third layer is a dropout layer with six regular neurons even though the program has dropped 50% of them. While the program drops these neurons, it neither calculates nor trains them. However, the final neural network will use all of these neurons for the output. As previously mentioned, the program only temporarily discards the neurons.\n",
    "During subsequent training iterations, the program chooses different sets of neurons from the dropout layer. Although we chose a probability of 50% for dropout, the computer will not necessarily drop three neurons. It is as if we flipped a coin for each of the dropout candidate neurons to choose if that neuron was dropped out. You must know that the program should never drop the bias neuron. Only the regular neurons on a dropout layer are candidates. The implementation of the training algorithm influences the process of discarding neurons. The dropout set frequently changes once per training iteration or batch. The program can also provide intervals where all neurons are present. Some neural network frameworks give additional hyper-parameters to allow you to specify exactly the rate of this interval.\n",
    "\n",
    "Why dropout is capable of decreasing overfitting is a common question. The answer is that dropout can reduce the chance of a codependency developing between two neurons. Two neurons that develop a codependency will not be able to operate effectively when one is dropped out. As a result, the neural network can no longer rely on the presence of every neuron, and it trains accordingly. This characteristic decreases its ability to memorize the information presented to it, thereby forcing generalization.\n",
    "\n",
    "Dropout also decreases overfitting by forcing a bootstrapping process upon the neural network. Bootstrapping is a very common ensemble technique. We will discuss ensembling in greater detail in Chapter 16, Modeling with Neural Networks. Basically, ensembling is a technique of machine learning that combines multiple models to produce a better result than those achieved by individual models. Ensemble is a term that originates from the musical ensembles in which the final music product that the audience hears is the combination of many instruments.\n",
    "Bootstrapping is one of the most simple ensemble techniques. The programmer using bootstrapping simply trains a number of neural networks to perform exactly the same task. However, each of these neural networks will perform differently because of some training techniques and the random numbers used in the neural network weight initialization. The difference in weights causes the performance variance. The output from this ensemble of neural networks becomes the average output of the members taken together. This process decreases overfitting through the consensus of differently trained neural networks.\n",
    "\n",
    "Dropout works somewhat like bootstrapping. You might think of each neural network that results from a different set of neurons being dropped out as an individual member in an ensemble. As training progresses, the program creates more neural networks in this way. However, dropout does not require the same amount of processing as does bootstrapping. The new neural networks created are temporary; they exist only for a training iteration. The final result is also a single neural network, rather than an ensemble of neural networks to be averaged together.\n",
    "\n",
    "Here link explaining graphically this method: https://yusugomori.com/projects/deep-learning/dropout-relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data set\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Read the data set\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Generate dummies for job\n",
    "df = pd.concat([df,pd.get_dummies(df['job'],prefix=\"job\")],axis=1)\n",
    "df.drop('job', axis=1, inplace=True)\n",
    "\n",
    "# Generate dummies for area\n",
    "df = pd.concat([df,pd.get_dummies(df['area'],prefix=\"area\")],axis=1)\n",
    "df.drop('area', axis=1, inplace=True)\n",
    "\n",
    "# Missing values for income\n",
    "med = df['income'].median()\n",
    "df['income'] = df['income'].fillna(med)\n",
    "\n",
    "# Standardize ranges\n",
    "df['income'] = zscore(df['income'])\n",
    "df['aspect'] = zscore(df['aspect'])\n",
    "df['save_rate'] = zscore(df['save_rate'])\n",
    "df['age'] = zscore(df['age'])\n",
    "df['subscriptions'] = zscore(df['subscriptions'])\n",
    "\n",
    "# Convert to numpy - Classification\n",
    "x_columns = df.columns.drop('product').drop('id')\n",
    "x = df[x_columns].values\n",
    "dummies = pd.get_dummies(df['product']) # Classification\n",
    "products = dummies.columns\n",
    "y = dummies.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "Fold score (accuracy): 0.6875\n",
      "Fold #2\n",
      "Fold score (accuracy): 0.7175\n",
      "Fold #3\n",
      "Fold score (accuracy): 0.69\n",
      "Fold #4\n",
      "Fold score (accuracy): 0.72\n",
      "Fold #5\n",
      "Fold score (accuracy): 0.6825\n",
      "Final score (accuracy): 0.6995\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# Keras with dropout for Classification\n",
    "########################################\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Cross-validate\n",
    "kf = KFold(5, shuffle=True, random_state=42)\n",
    "    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "for train, test in kf.split(x):\n",
    "    fold+=1\n",
    "    print(f\"Fold #{fold}\")\n",
    "        \n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    #kernel_regularizer=regularizers.l2(0.01),\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(25, activation='relu', \\\n",
    "                activity_regularizer=regularizers.l1(1e-4))) # Hidden 2\n",
    "    # Usually do not add dropout after final hidden layer\n",
    "    #model.add(Dropout(0.5)) \n",
    "    model.add(Dense(y.shape[1],activation='softmax')) # Output\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),\\\n",
    "              verbose=0,epochs=500)\n",
    "    \n",
    "    pred = model.predict(x_test)\n",
    "    \n",
    "    oos_y.append(y_test)\n",
    "    # raw probabilities to chosen class (highest probability)\n",
    "    pred = np.argmax(pred,axis=1) \n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure this fold's accuracy\n",
    "    y_compare = np.argmax(y_test,axis=1) # For accuracy calculation\n",
    "    score = metrics.accuracy_score(y_compare, pred)\n",
    "    print(f\"Fold score (accuracy): {score}\")\n",
    "\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "oos_y_compare = np.argmax(oos_y,axis=1) # For accuracy calculation\n",
    "\n",
    "score = metrics.accuracy_score(oos_y_compare, oos_pred)\n",
    "print(f\"Final score (accuracy): {score}\")    \n",
    "    \n",
    "# Write the cross-validated prediction\n",
    "oos_y = pd.DataFrame(oos_y)\n",
    "oos_pred = pd.DataFrame(oos_pred)\n",
    "oosDF = pd.concat( [df, oos_y, oos_pred],axis=1 )\n",
    "#oosDF.to_csv(filename_write,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5.5: Benchmarking Regularization Techniques\n",
    "\n",
    "Quite a few hyperparameters have been introduced so far.  Tweaking each of these values can have an effect on the score obtained by your neural networks.  Some of the hyperparameters seen so far include:\n",
    "\n",
    "* Number of layers in the neural network\n",
    "* How many neurons in each layer\n",
    "* What activation functions to use on each layer\n",
    "* Dropout percent on each layer\n",
    "* L1 and L2 values on each layer\n",
    "\n",
    "To try out each of these hyperparameters you will need to run train neural networks with multiple settings for each hyperparameter.  However, you may have noticed that neural networks often produce somewhat different results when trained multiple times.  This is because the neural networks start with random weights.  Because of this it is necessary to fit and evaluate a neural network times to ensure that one set of hyperparameters are actually better than another.  Bootstrapping can be an effective means of benchmarking (comparing) two sets of hyperparameters.  \n",
    "\n",
    "Bootstrapping is similar to cross-validation.  Both go through a number of cycles/folds providing validation and training sets.  However, bootstrapping can have an unlimited number of cycles.  Bootstrapping chooses a new train and validation split each cycle, with replacement.  The fact that each cycle is chosen with replacement means that, unlike cross validation, there will often be repeated rows selected between cycles.  If you run the bootstrap for enough cycles, there will be duplicate cycles.\n",
    "\n",
    "In this part we will use bootstrapping for hyperparameter benchmarking.  We will train a neural network for a specified number of splits (denoted by the SPLITS constant).  For these examples we use 100.  We will compare the average score at the end of the 100.  By the end of the cycles the mean score will have converged somewhat.  This ending score will be a much better basis of comparison than a single cross-validation.  Additionally, the average number of epochs will be tracked to give an idea of a possible optimal value.  Because the early stopping validation set is also used to evaluate the the neural network as well, it might be slightly inflated.  This is because we are both stopping and evaluating on the same sample.  However, we are using the scores only as relative measures to determine the superiority of one set of hyperparameters to another, so this slight inflation should not present too much of a problem.\n",
    "\n",
    "Because we are benchmarking, we will display the amount of time taken for each cycle.  The following function can be used to nicely format a time span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nicely formatted time string\n",
    "\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping for Regression\n",
    "\n",
    "Regression bootstrapping uses the **ShuffleSplit** object to perform the splits.  This is similar to **KFold** for cross validation, no balancing takes place.  To demonstrate this technique we will attempt to predict the age column for the jh-simple-dataset this data is loaded by the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data set\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Generate dummies for job\n",
    "df = pd.concat([df,pd.get_dummies(df['job'],prefix=\"job\")],axis=1)\n",
    "df.drop('job', axis=1, inplace=True)\n",
    "\n",
    "# Generate dummies for area\n",
    "df = pd.concat([df,pd.get_dummies(df['area'],prefix=\"area\")],axis=1)\n",
    "df.drop('area', axis=1, inplace=True)\n",
    "\n",
    "# Generate dummies for product\n",
    "df = pd.concat([df,pd.get_dummies(df['product'],prefix=\"product\")],axis=1)\n",
    "df.drop('product', axis=1, inplace=True)\n",
    "\n",
    "# Missing values for income\n",
    "med = df['income'].median()\n",
    "df['income'] = df['income'].fillna(med)\n",
    "\n",
    "# Standardize ranges\n",
    "df['income'] = zscore(df['income'])\n",
    "df['aspect'] = zscore(df['aspect'])\n",
    "df['save_rate'] = zscore(df['save_rate'])\n",
    "df['subscriptions'] = zscore(df['subscriptions'])\n",
    "\n",
    "# Convert to numpy - Classification\n",
    "x_columns = df.columns.drop('age').drop('id')\n",
    "x = df[x_columns].values\n",
    "y = df['age'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code performs the bootstrap.  The architecture of the neural \n",
    "network can be adjusted to compare many different configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import statistics\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "SPLITS = 50\n",
    "\n",
    "# Bootstrap\n",
    "boot = ShuffleSplit(n_splits=SPLITS, test_size=0.1, random_state=42)\n",
    "\n",
    "# Track progress\n",
    "mean_benchmark = []\n",
    "epochs_needed = []\n",
    "num = 0\n",
    "\n",
    "# Loop through samples\n",
    "for train, test in boot.split(x):\n",
    "    start_time = time.time()\n",
    "    num+=1\n",
    "\n",
    "    # Split train and test\n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "\n",
    "    # Construct neural network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=x_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
    "        patience=5, verbose=0, mode='auto', restore_best_weights=True)\n",
    "\n",
    "    # Train on the bootstrap sample\n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),\n",
    "              callbacks=[monitor],verbose=0,epochs=1000)\n",
    "    epochs = monitor.stopped_epoch\n",
    "    epochs_needed.append(epochs)\n",
    "    \n",
    "    # Predict on the out of boot (validation)\n",
    "    pred = model.predict(x_test)\n",
    "  \n",
    "    # Measure this bootstrap's log loss\n",
    "    score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "    mean_benchmark.append(score)\n",
    "    m1 = statistics.mean(mean_benchmark)\n",
    "    m2 = statistics.mean(epochs_needed)\n",
    "    mdev = statistics.pstdev(mean_benchmark)\n",
    "    \n",
    "    # Record this iteration\n",
    "    time_took = time.time() - start_time\n",
    "    print(f\"#{num}: score={score:.6f}, mean score={m1:.6f}, stdev={mdev:.6f}, \n",
    "          epochs={epochs}, mean epochs={int(m2)}, \n",
    "          time={hms_string(time_took)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
